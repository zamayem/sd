{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DOWNLOAD MINICONDA\n",
    "# !wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n",
    "# # INSTALL MINICONDA\n",
    "# bash ~/miniconda.sh -b -u -p ~/miniconda3\n",
    "# # REMOVE MINICONDA INSTALLER\n",
    "# rm -rf ~/miniconda.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local pipeline\n",
    "# To load a diffusion pipeline locally, use git-lfs to manually download the checkpoint (in this case, runwayml/stable-diffusion-v1-5) to your local disk. This creates a local folder, ./stable-diffusion-v1-5, on your disk:\n",
    "\n",
    "\n",
    "### INSTALL GIT LFS\n",
    "# git-lfs install\n",
    "\n",
    "### DOWNLOAD MODEL\n",
    "## REMEMBER TO DELETE .GIT WHEN DONE - HOGS STORAGE AND IS ONLY NEEDED FOR VERSION CONTROL\n",
    "# git clone https://huggingface.co/runwayml/stable-diffusion-v1-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE CONDA ENVIRONMENT\n",
    "# !conda create -n diffusers_test python=3.11 -y\n",
    "### EXPORT ENVIRONMENT\n",
    "# !conda env export > diffusers_test.yml\n",
    "### CREATE ENVIRONMENT FROM FILE\n",
    "# !conda env create -f diffuser_environment/diffusers_test.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOWNLOAD REPO\n",
    "# !git clone https://github.com/huggingface/diffusers\n",
    "# !cd diffusers && git clone https://huggingface.co/stabilityai/sd-vae-ft-mse-original\n",
    "### DOWNLOAD FILE FROM SS\n",
    "# !wget https://raw.githubusercontent.com/ShivamShrirao/diffusers/main/examples/dreambooth/train_dreambooth.py -O ss_train_dreambooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### install dependencies\n",
    "# !ls ../diffusers\n",
    "# !cd ../diffusers \\\n",
    "# && pip install . \\\n",
    "# && cd examples/dreambooth \\\n",
    "# && pip install -r requirements.txt \\\n",
    "# && pip install -r requirements_flax.txt \\\n",
    "# && pip install natsort==8.3.1 \\\n",
    "# && pip install matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET ACCELERATE CONFIG\n",
    "\n",
    "# !accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RUNNING A MODEL #####\n",
    "\n",
    "# from diffusers import DiffusionPipeline\n",
    "# import torch\n",
    "# # pro = \"\"\"ultra realistic full body portrait, blue eyes,  hyper detail,\n",
    "# #  cinematic lightingCanon EOS R3, nikon, f/1.4, ISO 200, 1/160s, 8K, RAW, unedited\n",
    "# #  , symmetrical balance, in-frame, 8K\"\"\"\n",
    "# pro = \"\"\"A photo of dog in a bucket\"\"\"\n",
    "# neg_pro = \"\"\"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly,\n",
    "#  blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy,\n",
    "# double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs \"\"\"\n",
    "# repo_id = '/home/emizam/github/diffusers/realistic-vision-v51'\n",
    "# stable_diffusion = DiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n",
    "# stable_diffusion.to(\"cuda\")\n",
    "# # stable_diffusion.enable_xformers_memory_efficient_attention()\n",
    "# # stable_diffusion.enable_model_cpu_offload\n",
    "# stable_diffusion(prompt=pro,\n",
    "# negative_prompt=neg_pro,num_inference_steps=50).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CONCEPT LIST ####\n",
    "\n",
    "\n",
    "# Configure a list of concepts to finetune on top of the normal StableDiffusion model\n",
    "# For this example, we will only use 1 new concept - but you can add multiple concepts here and tweak '--max_training_steps' accordingly\n",
    "\n",
    "# -instance_prompt - the prompt we would type to generate the image we are attempting to fine tune\n",
    "\n",
    "# -class_prompt - denotes a prompt without the unique identifier/instance. This prompt is used for generating \"class images\" for prior preservation. For our example, this prompt is - \"a photo of a person\" versus a photo of a specific person.\n",
    "\n",
    "# -instance_data_dir - the location where our training images are stored for finetuning\n",
    "\n",
    "# -class_data_dir - sample images for the general class of prompt we are fine tuning - if there are no images here, samples will be generated. Otherwise, you can provie ~20 images of the general concept you want to generate (but not the actual instance images that we finetune on)\n",
    "\n",
    "\n",
    "### https://github.com/ccrngd1/StableDiffusionExperimenting/blob/main/ShivamShriraoSD%2BDM.ipynb\n",
    "# concepts_list = [ \n",
    "#     {\n",
    "#         \"instance_prompt\": \"photo of cc dog\",\n",
    "#         \"class_prompt\": \"photo of a dog\",\n",
    "#         \"instance_data_dir\": \"./content/data/cc\",\n",
    "#         \"class_data_dir\": \"./content/data/person\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# import json\n",
    "# import os\n",
    "# for c in concepts_list:\n",
    "#     os.makedirs(c[\"instance_data_dir\"], exist_ok=True)\n",
    "\n",
    "# with open(\"concepts_list_cc.json\", \"w\") as f:\n",
    "#     json.dump(concepts_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FINE TUNING A MODEL #####\n",
    "\n",
    "# MODEL_NAME = \"/home/emizam/github/diffusers/stable-diffusion-2-base\"\n",
    "# VAE_NAME= \"/home/emizam/github/diffusers/sd-vae-ft-mse\"\n",
    "# INSTANCE_DIR=\"/home/emizam/github/diffusers/playground/dog\"\n",
    "# OUTPUT_DIR=\"./output\"\n",
    "# PRECISION = \"fp16\"\n",
    "\n",
    "# !accelerate launch /home/emizam/github/diffusers_environment/ss_train_dreambooth.py\\\n",
    "#   --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "#   --pretrained_vae_name_or_path=$VAE_NAME \\\n",
    "#   --instance_data_dir=$INSTANCE_DIR \\\n",
    "#   --seed=1337 \\\n",
    "#   --output_dir=$OUTPUT_DIR \\\n",
    "#   --instance_prompt=\"a photo of sks dog\" \\\n",
    "#   --resolution=512 \\\n",
    "#   --train_batch_size=1 \\\n",
    "#   --gradient_accumulation_steps=1 \\\n",
    "#   --learning_rate=5e-6 \\\n",
    "#   --lr_scheduler=\"constant\" \\\n",
    "#   --lr_warmup_steps=0 \\\n",
    "#   --max_train_steps=10 \\\n",
    "# #   --save_sample_prompt=\"photo of cc person\" \\\n",
    "#   --revision=$PRECISION \\\n",
    "#   --num_class_images=50 \\\n",
    "# #   --concepts_list=\"concepts_list_cc.json\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update weights\n",
    "\n",
    "# WEIGHTS_DIR = \"\" \n",
    "# if WEIGHTS_DIR == \"\":\n",
    "#     from natsort import natsorted\n",
    "#     from glob import glob\n",
    "#     import os\n",
    "#     WEIGHTS_DIR = natsorted(glob(OUTPUT_DIR + os.sep + \"*\"))[-1]\n",
    "# print(f\"[*] WEIGHTS_DIR={WEIGHTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate a grid of preview images\n",
    "\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.image as mpimg\n",
    "\n",
    "# weights_folder = OUTPUT_DIR\n",
    "# folders = sorted([f for f in os.listdir(weights_folder) if f != \"0\"], key=lambda x: int(x))\n",
    "\n",
    "# row = len(folders)\n",
    "# col = len(os.listdir(os.path.join(weights_folder, folders[0], \"samples\")))\n",
    "# scale = 4\n",
    "# fig, axes = plt.subplots(row, col, figsize=(col*scale, row*scale), gridspec_kw={'hspace': 0, 'wspace': 0})\n",
    "\n",
    "# for i, folder in enumerate(folders):\n",
    "#     folder_path = os.path.join(weights_folder, folder)\n",
    "#     image_folder = os.path.join(folder_path, \"samples\")\n",
    "#     images = [f for f in os.listdir(image_folder)]\n",
    "#     for j, image in enumerate(images):\n",
    "#         if row == 1:\n",
    "#             currAxes = axes[j]\n",
    "#         else:\n",
    "#             currAxes = axes[i, j]\n",
    "#         if i == 0:\n",
    "#             currAxes.set_title(f\"Image {j}\")\n",
    "#         if j == 0:\n",
    "#             currAxes.text(-0.1, 0.5, folder, rotation=0, va='center', ha='center', transform=currAxes.transAxes)\n",
    "#         image_path = os.path.join(image_folder, image)\n",
    "#         img = mpimg.imread(image_path)\n",
    "#         currAxes.imshow(img, cmap='gray')\n",
    "#         currAxes.axis('off')\n",
    "        \n",
    "# plt.tight_layout()\n",
    "# plt.savefig('content/grid.png', dpi=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Convert weights to ckpt to use in web UIs like AutoMatic1111\n",
    "\n",
    "# ckpt_path = WEIGHTS_DIR + \"/model.ckpt\"\n",
    "\n",
    "# half_arg = \"\"\n",
    "\n",
    "# fp16 = True #@param {type: \"boolean\"}\n",
    "\n",
    "# if fp16:\n",
    "#     half_arg = \"--half\"\n",
    "# !python ../diffusers/scripts/convert_diffusers_to_original_stable_diffusion.py --model_path $WEIGHTS_DIR  --checkpoint_path $ckpt_path $half_arg\n",
    "# print(f\"[*] Converted ckpt saved at {ckpt_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
