{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-30 12:12:05--  https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
      "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
      "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 141613749 (135M) [text/x-sh]\n",
      "Saving to: ‘miniconda.sh’\n",
      "\n",
      "miniconda.sh         39%[======>             ]  52.93M  16.3MB/s    eta 5s     ^C\n"
     ]
    }
   ],
   "source": [
    "# # DOWNLOAD MINICONDA\n",
    "# !wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh\n",
    "# # INSTALL MINICONDA\n",
    "# bash ~/miniconda.sh -b -u -p ~/miniconda3\n",
    "# # REMOVE MINICONDA INSTALLER\n",
    "# rm -rf ~/miniconda.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local pipeline\n",
    "# To load a diffusion pipeline locally, use git-lfs to manually download the checkpoint (in this case, runwayml/stable-diffusion-v1-5) to your local disk. This creates a local folder, ./stable-diffusion-v1-5, on your disk:\n",
    "\n",
    "\n",
    "### INSTALL GIT LFS\n",
    "# git-lfs install\n",
    "\n",
    "### DOWNLOAD MODEL\n",
    "## REMEMBER TO DELETE .GIT WHEN DONE - HOGS STORAGE AND IS ONLY NEEDED FOR VERSION CONTROL\n",
    "# git clone https://huggingface.co/runwayml/stable-diffusion-v1-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE CONDA ENVIRONMENT\n",
    "# !conda create -n diffusers_test python=3.11 -y\n",
    "### EXPORT ENVIRONMENT\n",
    "# !conda env export > diffusers_test.yml\n",
    "### CREATE ENVIRONMENT FROM FILE\n",
    "# !conda env create -f diffusers_test.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOWNLOAD REPO\n",
    "# !git clone https://github.com/huggingface/diffusers\n",
    "# !cd diffusers && git clone https://huggingface.co/stabilityai/sd-vae-ft-mse-original\n",
    "### DOWNLOAD FILE FROM SS\n",
    "# !wget https://raw.githubusercontent.com/ShivamShrirao/diffusers/main/examples/dreambooth/train_dreambooth.py -O ss_train_dreambooth.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### install dependencies\n",
    "\n",
    "# !cd diffusers && pip install . && cd examples/dreambooth && pip install -r requirements.txt && pip install -r requirements_flax.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET ACCELERATE CONFIG\n",
    "\n",
    "# !accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RUNNING A MODEL #####\n",
    "\n",
    "# from diffusers import DiffusionPipeline\n",
    "# import torch\n",
    "# # pro = \"\"\"ultra realistic full body portrait, blue eyes,  hyper detail,\n",
    "# #  cinematic lightingCanon EOS R3, nikon, f/1.4, ISO 200, 1/160s, 8K, RAW, unedited\n",
    "# #  , symmetrical balance, in-frame, 8K\"\"\"\n",
    "# pro = \"\"\"A photo of dog in a bucket\"\"\"\n",
    "# neg_pro = \"\"\"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly,\n",
    "#  blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy,\n",
    "# double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs \"\"\"\n",
    "# repo_id = '/home/emizam/github/diffusers/realistic-vision-v51'\n",
    "# stable_diffusion = DiffusionPipeline.from_pretrained(repo_id, use_safetensors=True)\n",
    "# stable_diffusion.to(\"cuda\")\n",
    "# # stable_diffusion.enable_xformers_memory_efficient_attention()\n",
    "# # stable_diffusion.enable_model_cpu_offload\n",
    "# stable_diffusion(prompt=pro,\n",
    "# negative_prompt=neg_pro,num_inference_steps=50).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CONCEPT LIST ####\n",
    "\n",
    "\n",
    "# Configure a list of concepts to finetune on top of the normal StableDiffusion model\n",
    "# For this example, we will only use 1 new concept - but you can add multiple concepts here and tweak '--max_training_steps' accordingly\n",
    "\n",
    "# -instance_prompt - the prompt we would type to generate the image we are attempting to fine tune\n",
    "\n",
    "# -class_prompt - denotes a prompt without the unique identifier/instance. This prompt is used for generating \"class images\" for prior preservation. For our example, this prompt is - \"a photo of a person\" versus a photo of a specific person.\n",
    "\n",
    "# -instance_data_dir - the location where our training images are stored for finetuning\n",
    "\n",
    "# -class_data_dir - sample images for the general class of prompt we are fine tuning - if there are no images here, samples will be generated. Otherwise, you can provie ~20 images of the general concept you want to generate (but not the actual instance images that we finetune on)\n",
    "\n",
    "\n",
    "### https://github.com/ccrngd1/StableDiffusionExperimenting/blob/main/ShivamShriraoSD%2BDM.ipynb\n",
    "# [\n",
    "#     {\n",
    "#         \"instance_prompt\": \"photo of cc person\",\n",
    "#         \"class_prompt\": \"photo of a person\",\n",
    "#         \"instance_data_dir\": \"./content/data/cc\",\n",
    "#         \"class_data_dir\": \"./content/data/person\"\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FINE TUNING A MODEL #####\n",
    "\n",
    "# MODEL_NAME = \"/home/emizam/github/diffusers/stable-diffusion-2-base\"\n",
    "# VAE_NAME= \"/home/emizam/github/diffusers/sd-vae-ft-mse\"\n",
    "# INSTANCE_DIR=\"/home/emizam/github/diffusers/playground/dog\"\n",
    "# OUTPUT_DIR=\"./output\"\n",
    "# PRECISION = \"fp16\"\n",
    "\n",
    "# !accelerate launch /home/emizam/github/diffusers_environment/ss_train_dreambooth.py\\\n",
    "#   --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "#   --pretrained_vae_name_or_path=$VAE_NAME \\\n",
    "#   --instance_data_dir=$INSTANCE_DIR \\\n",
    "#   --output_dir=$OUTPUT_DIR \\\n",
    "#   --instance_prompt=\"a photo of sks dog\" \\\n",
    "#   --resolution=512 \\\n",
    "#   --train_batch_size=1 \\\n",
    "#   --gradient_accumulation_steps=1 \\\n",
    "#   --learning_rate=5e-6 \\\n",
    "#   --lr_scheduler=\"constant\" \\\n",
    "#   --lr_warmup_steps=0 \\\n",
    "#   --max_train_steps=10 \\\n",
    "# #   --save_sample_prompt=\"photo of cc person\" \\\n",
    "#   --revision=$PRECISION \\\n",
    "#   --num_class_images=50 \\\n",
    "# #   --concepts_list=\"concepts_list_cc.json\"    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
